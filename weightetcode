import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.base import BaseEstimator

# Upload the Excel file
from google.colab import files
uploaded = files.upload()

scaler = StandardScaler()
df = pd.read_excel('Masters_2021.xlsx', skiprows=1)

# Split the data into training set and test set
train_df, test_df = train_test_split(df, test_size=0.2)

columns_to_keep = ['player_name', 'sg_putt',
                   'sg_arg', 'sg_app', 'sg_ott', 'sg_t2g', 'sg_total', 'driving_dist',
                   'driving_acc', 'gir', 'scrambling', 'prox_rgh', 'prox_fw',
                   'great_shots', 'poor_shots', 'round_score']

# List of columns to drop
columns_to_drop = [col for col in df.columns if col not in columns_to_keep]

# Drop the columns
train_data = train_df.drop(columns_to_drop, axis=1)
test_data = test_df.drop(columns_to_drop, axis=1)

# For the training data
train_data.fillna(train_data.mean(), inplace=True)

# For the testing data
test_data.fillna(test_data.mean(), inplace=True)

# Extract player names
train_players = train_data['player_name']
test_players = test_data['player_name']

# Concatenate training and test player names
all_players = pd.concat([train_players, test_players])

# Create dictionary for all player names
player_name_to_idx = {name: idx for idx, name in enumerate(all_players.unique())}

# Select the rest of the columns as features
train_X = train_data.drop(['player_name', 'round_score'], axis=1)
test_X = test_data.drop(['player_name', 'round_score'], axis=1)

# Select round scores as targets
train_Y = train_data['round_score']
test_Y = test_data['round_score']

train_data.to_csv('train_data.csv', index=False)
test_data.to_csv('test_data.csv', index=False)

# Feature Engineering: Add interaction terms and polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
train_X_poly = poly.fit_transform(train_X)
test_X_poly = poly.transform(test_X)

# Normalization: Try a different normalization technique
# Here, we will use Min-Max scaling
min_max_scaler = MinMaxScaler()
train_X_scaled = min_max_scaler.fit_transform(train_X_poly)
test_X_scaled = min_max_scaler.transform(test_X_poly)

# Convert scaled data to PyTorch tensors
train_X_tensor = torch.tensor(train_X_scaled, dtype=torch.float32)
train_Y_tensor = torch.tensor(train_Y.values.reshape(-1, 1), dtype=torch.float32)
test_X_tensor = torch.tensor(test_X_scaled, dtype=torch.float32)
test_Y_tensor = torch.tensor(test_Y.values.reshape(-1, 1), dtype=torch.float32)

# Define a wrapper class for the neural network
class PyTorchRegressor(BaseEstimator):
    def __init__(self, model):
        self.model = model

    def fit(self, X, y):
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.model.lr)

        for epoch in range(self.model.epochs):
            optimizer.zero_grad()
            outputs = self.model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()

            if epoch % 10 == 0:
                print(f'Epoch [{epoch+1}/{self.model.epochs}], Loss: {loss.item():.4f}')

        return self

    def predict(self, X):
        self.model.eval()
        with torch.no_grad():
            predicted = self.model(X).cpu().numpy().flatten()
        return predicted

# Define a neural network with dropout layers
class RegressionNetWithDropout(nn.Module):
    def __init__(self, input_size, lr=0.001, epochs=100):
        super(RegressionNetWithDropout, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(128, 64)
        self.dropout2 = nn.Dropout(0.2)
        self.fc3 = nn.Linear(64, 1)
        self.lr = lr
        self.epochs = epochs

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

    def set_params(self, **params):
        if 'lr' in params:
            self.lr = params['lr']
        if 'epochs' in params:
            self.epochs = params['epochs']
        return self

# Create an instance of the wrapper class with the neural network
model = PyTorchRegressor(RegressionNetWithDropout(input_size=train_X_tensor.shape[1]))

# Grid search for hyperparameter tuning
param_grid = {
    'model__lr': [0.001, 0.01, 0.1],
    'model__epochs': [100, 200, 300]
}

grid_search = GridSearchCV(estimator=model,
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',
                           cv=3)

grid_search.fit(train_X_tensor, train_Y_tensor)

# Get the best model from grid search
best_model = grid_search.best_estimator_

# Evaluate the best model
predicted_means = best_model.predict(test_X_tensor)

# Calculate and print the Mean Squared Error (MSE)
mse = mean_squared_error(test_Y, predicted_means)
print(f'Mean Squared Error (MSE): {mse}')

# Combine player names with predicted and actual scores
result_df = pd.DataFrame({'player_name': test_players, 'Predicted_Score': predicted_means, 'Actual_Score': test_Y})
result_df.reset_index(drop=True, inplace=True)

# Print players' names along with predicted and actual scores
for index, row in result_df.iterrows():
    print(f"Player: {row['player_name']}, Predicted Score: {row['Predicted_Score']}, Actual Score: {row['Actual_Score']}")
